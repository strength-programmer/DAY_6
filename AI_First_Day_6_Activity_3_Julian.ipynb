{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_4JjD-q4sde0",
        "outputId": "c34c6dea-2d8a-44fe-9f88-66492a30feec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/openai/swarm.git\n",
            "  Cloning https://github.com/openai/swarm.git to /tmp/pip-req-build-fkg5p3vb\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/swarm.git /tmp/pip-req-build-fkg5p3vb\n",
            "  Resolved https://github.com/openai/swarm.git to commit 9db581cecaacea0d46a933d6453c312b034dbf47\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from swarm==0.1.0) (1.26.4)\n",
            "Requirement already satisfied: openai>=1.33.0 in /usr/local/lib/python3.10/dist-packages (from swarm==0.1.0) (1.54.4)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.10/dist-packages (from swarm==0.1.0) (8.3.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from swarm==0.1.0) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from swarm==0.1.0) (4.66.6)\n",
            "Collecting pre-commit (from swarm==0.1.0)\n",
            "  Downloading pre_commit-4.0.1-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting instructor (from swarm==0.1.0)\n",
            "  Downloading instructor-1.6.4-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.33.0->swarm==0.1.0) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.33.0->swarm==0.1.0) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.33.0->swarm==0.1.0) (0.27.2)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.33.0->swarm==0.1.0) (0.7.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.33.0->swarm==0.1.0) (2.9.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai>=1.33.0->swarm==0.1.0) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai>=1.33.0->swarm==0.1.0) (4.12.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.9.1 in /usr/local/lib/python3.10/dist-packages (from instructor->swarm==0.1.0) (3.11.2)\n",
            "Requirement already satisfied: docstring-parser<0.17,>=0.16 in /usr/local/lib/python3.10/dist-packages (from instructor->swarm==0.1.0) (0.16)\n",
            "Requirement already satisfied: jinja2<4.0.0,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from instructor->swarm==0.1.0) (3.1.4)\n",
            "Collecting jiter<1,>=0.4.0 (from openai>=1.33.0->swarm==0.1.0)\n",
            "  Downloading jiter-0.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: pydantic-core<3.0.0,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from instructor->swarm==0.1.0) (2.23.4)\n",
            "Requirement already satisfied: rich<14.0.0,>=13.7.0 in /usr/local/lib/python3.10/dist-packages (from instructor->swarm==0.1.0) (13.9.4)\n",
            "Requirement already satisfied: tenacity<10.0.0,>=9.0.0 in /usr/local/lib/python3.10/dist-packages (from instructor->swarm==0.1.0) (9.0.0)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from instructor->swarm==0.1.0) (0.13.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->swarm==0.1.0) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->swarm==0.1.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->swarm==0.1.0) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->swarm==0.1.0) (2024.8.30)\n",
            "Collecting cfgv>=2.0.0 (from pre-commit->swarm==0.1.0)\n",
            "  Downloading cfgv-3.4.0-py2.py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting identify>=1.0.0 (from pre-commit->swarm==0.1.0)\n",
            "  Downloading identify-2.6.2-py2.py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting nodeenv>=0.11.1 (from pre-commit->swarm==0.1.0)\n",
            "  Downloading nodeenv-1.9.1-py2.py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from pre-commit->swarm==0.1.0) (6.0.2)\n",
            "Collecting virtualenv>=20.10.0 (from pre-commit->swarm==0.1.0)\n",
            "  Downloading virtualenv-20.27.1-py3-none-any.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.10/dist-packages (from pytest->swarm==0.1.0) (2.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from pytest->swarm==0.1.0) (24.2)\n",
            "Requirement already satisfied: pluggy<2,>=1.5 in /usr/local/lib/python3.10/dist-packages (from pytest->swarm==0.1.0) (1.5.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /usr/local/lib/python3.10/dist-packages (from pytest->swarm==0.1.0) (1.2.2)\n",
            "Requirement already satisfied: tomli>=1 in /usr/local/lib/python3.10/dist-packages (from pytest->swarm==0.1.0) (2.1.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.1->instructor->swarm==0.1.0) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.1->instructor->swarm==0.1.0) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.1->instructor->swarm==0.1.0) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.1->instructor->swarm==0.1.0) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.1->instructor->swarm==0.1.0) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.1->instructor->swarm==0.1.0) (0.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.1->instructor->swarm==0.1.0) (1.17.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.1->instructor->swarm==0.1.0) (4.0.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai>=1.33.0->swarm==0.1.0) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>=1.33.0->swarm==0.1.0) (0.14.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2<4.0.0,>=3.1.4->instructor->swarm==0.1.0) (3.0.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai>=1.33.0->swarm==0.1.0) (0.7.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0.0,>=13.7.0->instructor->swarm==0.1.0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0.0,>=13.7.0->instructor->swarm==0.1.0) (2.18.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.9.0->instructor->swarm==0.1.0) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.9.0->instructor->swarm==0.1.0) (1.5.4)\n",
            "Collecting distlib<1,>=0.3.7 (from virtualenv>=20.10.0->pre-commit->swarm==0.1.0)\n",
            "  Downloading distlib-0.3.9-py2.py3-none-any.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: filelock<4,>=3.12.2 in /usr/local/lib/python3.10/dist-packages (from virtualenv>=20.10.0->pre-commit->swarm==0.1.0) (3.16.1)\n",
            "Requirement already satisfied: platformdirs<5,>=3.9.1 in /usr/local/lib/python3.10/dist-packages (from virtualenv>=20.10.0->pre-commit->swarm==0.1.0) (4.3.6)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=13.7.0->instructor->swarm==0.1.0) (0.1.2)\n",
            "Downloading instructor-1.6.4-py3-none-any.whl (70 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m70.1/70.1 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pre_commit-4.0.1-py2.py3-none-any.whl (218 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m218.7/218.7 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cfgv-3.4.0-py2.py3-none-any.whl (7.2 kB)\n",
            "Downloading identify-2.6.2-py2.py3-none-any.whl (98 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m99.0/99.0 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jiter-0.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (325 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m325.2/325.2 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nodeenv-1.9.1-py2.py3-none-any.whl (22 kB)\n",
            "Downloading virtualenv-20.27.1-py3-none-any.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading distlib-0.3.9-py2.py3-none-any.whl (468 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m469.0/469.0 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: swarm\n",
            "  Building wheel for swarm (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for swarm: filename=swarm-0.1.0-py3-none-any.whl size=25999 sha256=b503638d6d3b0a712fcbd833855ac03d58d7b71f8ec51b0ffc9252c38eb1f4b6\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-20pnv7i8/wheels/46/9a/f7/7b8bbb674ae80ef0f62a632706c2c4cdfcf708e4da32e4e256\n",
            "Successfully built swarm\n",
            "Installing collected packages: distlib, virtualenv, nodeenv, jiter, identify, cfgv, pre-commit, instructor, swarm\n",
            "  Attempting uninstall: jiter\n",
            "    Found existing installation: jiter 0.7.1\n",
            "    Uninstalling jiter-0.7.1:\n",
            "      Successfully uninstalled jiter-0.7.1\n",
            "Successfully installed cfgv-3.4.0 distlib-0.3.9 identify-2.6.2 instructor-1.6.4 jiter-0.6.1 nodeenv-1.9.1 pre-commit-4.0.1 swarm-0.1.0 virtualenv-20.27.1\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.54.4)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.2)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.6.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.9.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.6)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.23.4)\n",
            "Collecting firecrawl-py\n",
            "  Downloading firecrawl_py-1.5.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from firecrawl-py) (2.32.3)\n",
            "Collecting python-dotenv (from firecrawl-py)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Collecting websockets (from firecrawl-py)\n",
            "  Downloading websockets-14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.10/dist-packages (from firecrawl-py) (1.6.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->firecrawl-py) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->firecrawl-py) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->firecrawl-py) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->firecrawl-py) (2024.8.30)\n",
            "Downloading firecrawl_py-1.5.0-py3-none-any.whl (16 kB)\n",
            "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading websockets-14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (168 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m168.2/168.2 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: websockets, python-dotenv, firecrawl-py\n",
            "Successfully installed firecrawl-py-1.5.0 python-dotenv-1.0.1 websockets-14.1\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/openai/swarm.git\n",
        "!pip install openai\n",
        "!pip install firecrawl-py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ny-I-rWrs0BO"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from firecrawl import FirecrawlApp\n",
        "from swarm import Swarm, Agent\n",
        "from openai import OpenAI\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xbSeqn4vs_Vk"
      },
      "outputs": [],
      "source": [
        "app = FirecrawlApp(api_key=\"\")\n",
        "api = OpenAI(api_key=\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zKp3DAAQtYel"
      },
      "outputs": [],
      "source": [
        "client = Swarm(api)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CDtp8gu9tZPg"
      },
      "outputs": [],
      "source": [
        "def scrape_website(url):\n",
        "    \"\"\"Scrape a website using Firecrawl.\"\"\"\n",
        "    scrape_status = app.scrape_url(\n",
        "        url,\n",
        "        params={'formats': ['markdown']}\n",
        "    )\n",
        "    return scrape_status"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F3jAONsyt0On"
      },
      "outputs": [],
      "source": [
        "def handoff_to_copywriter():\n",
        "    \"\"\"Hand off the campaign idea to the copywriter agent.\"\"\"\n",
        "    return copywriter_agent\n",
        "\n",
        "def handoff_to_analyst():\n",
        "    \"\"\"Hand off the website content to the analyst agent.\"\"\"\n",
        "    return analyst_agent\n",
        "\n",
        "def handoff_to_campaign_idea():\n",
        "    \"\"\"Hand off the target audience and goals to the campaign idea agent.\"\"\"\n",
        "    return campaign_idea_agent\n",
        "\n",
        "def handoff_to_website_scraper():\n",
        "    \"\"\"Hand off the url to the website scraper agent.\"\"\"\n",
        "    return website_scraper_agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "45Jhq3wGt84a"
      },
      "outputs": [],
      "source": [
        "user_interface_agent = Agent(\n",
        "    name=\"User Interface Agent\",\n",
        "    instructions=\"You are a user interface agent that handles all interactions with the user. You need to always start with a URL that the user wants to create a marketing strategy for. Ask clarification questions if needed. Be concise.\",\n",
        "    functions=[handoff_to_website_scraper],\n",
        ")\n",
        "\n",
        "website_scraper_agent = Agent(\n",
        "    name=\"Website Scraper Agent\",\n",
        "    instructions=\"You are a website scraper agent specialized in scraping website content.\",\n",
        "    functions=[scrape_website, handoff_to_analyst],\n",
        ")\n",
        "\n",
        "analyst_agent = Agent(\n",
        "    name=\"Analyst Agent\",\n",
        "    instructions=\"You are an analyst agent that examines website content and provides insights for marketing strategies. Be concise.\",\n",
        "    functions=[handoff_to_copywriter],\n",
        ")\n",
        "\n",
        "copywriter_agent = Agent(\n",
        "    name=\"Copywriter Agent\",\n",
        "    model=\"gpt-4o-mini\",\n",
        "    instructions=\"You are a copywriter agent specialized in creating compelling marketing copy based on website content and campaign ideas. Be concise.\",\n",
        "    )\n",
        "\n",
        "copywriter_agent_women = Agent(\n",
        "    name=\"Women-Centric Copywriter Agent\",\n",
        "    model=\"gpt-4o-mini\",\n",
        "    instructions=\"You are a copywriter agent specialized in creating empowering and relatable marketing copy that resonates with women. Craft messages that inspire confidence, highlight inclusivity, and celebrate individuality. Focus on creating a warm, supportive tone while emphasizing the benefits of the product or service. Incorporate language that speaks to common experiences, values, and aspirations of women. Use calls to action that encourage community, self-care, and empowerment.\"\n",
        ")\n",
        "copywriter_agent_gen_z = Agent(\n",
        "    name=\"Gen Z Copywriter Agent\",\n",
        "    model=\"gpt-4o-mini\",\n",
        "    instructions= \"You are a copywriter agent specialized in creating vibrant, trendy, and authentic marketing copy that resonates with Gen Z. Use casual, conversational language and pop-culture references when appropriate. Focus on sustainability, individuality, and social impactâ€”values that align with this generation. Be playful and bold, incorporating emojis, hashtags, and memes sparingly to keep the tone fresh and relatable. Create calls to action that feel personal and genuine, inspiring immediate engagement\"\n",
        ")\n",
        "\n",
        "copywriter_agent_millennials = Agent(\n",
        "    name=\"Millennial Copywriter Agent\",\n",
        "    model=\"gpt-4o-mini\",\n",
        "    instructions= \"You are a copywriter agent specialized in creating relatable and practical marketing copy that resonates with millennials. Highlight convenience, quality, and experiences over material goods. Incorporate humor, nostalgia, and a sense of achievement in your messaging. Focus on solutions for busy lifestyles, financial savviness, and wellness. Create calls to action that emphasize practicality and inspire confidence in making informed choices.\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IRiFPlcTt-cy"
      },
      "outputs": [],
      "source": [
        "struct = [{\"role\": \"user\", \"content\": \"https://www.firecrawl.dev/\"}]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uGbP9oh3wqh0"
      },
      "outputs": [],
      "source": [
        "response = client.run(agent=user_interface_agent, messages=struct)\n",
        "struct.append({\"role\": \"assistant\", \"content\": response.messages[-1][\"content\"]})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vlxnHkTCxOo8",
        "outputId": "188fbad4-103d-446f-afc7-46446554aeb3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6\n"
          ]
        }
      ],
      "source": [
        "print(len(response.messages))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J-6gnG7tx5qX",
        "outputId": "96362101-4127-4ca4-f8d6-c9836829e760"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "None\n"
          ]
        }
      ],
      "source": [
        "print(response.messages[0][\"content\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9eXA_Hgux9ds"
      },
      "source": [
        "# The Articles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1htwFedwyxm1"
      },
      "outputs": [],
      "source": [
        "response = client.run(agent=copywriter_agent, messages=struct)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9uBXCTioyxV_",
        "outputId": "301c0f53-03e2-4dc8-8d25-81242c70dc2c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1\n"
          ]
        }
      ],
      "source": [
        "print(len(response.messages))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pH4pzW7l1YBv"
      },
      "source": [
        "# Copywriter Agent Response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a2XawApY1b-w",
        "outputId": "2b147b57-4ebb-4ccf-d5c5-f74a107e6a59"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The website \"Firecrawl\" provides specialized web scraping services designed for AI applications, transforming website data into clean, LLM-ready markdown format. It's an essential tool for engineers and developers working in AI, data science, and machine learning. Key offerings include dynamic content handling, compatibility with popular tools, and support for extensive data projects. Pricing is flexible, featuring a free tier for up to 500 pages and scalable plans for larger needs, including an enterprise option. The platform promotes its open-source philosophy and is backed by partnerships with recognized entities like NVIDIA and Bain.\n",
            "\n",
            "### Marketing Copy Suggestions:\n",
            "1. **Empower Your AI Projects**: \"Transform your data collection for AI with Firecrawlâ€”where robust web scraping meets seamless integration.\"\n",
            "2. **Open Source, Trusted Partners**: \"Join a community of innovatorsâ€”leverage our open-source model backed by industry leaders.\"\n",
            "3. **Dynamic Content Made Easy**: \"Navigate the complexities of modern web scraping effortlessly, capturing your data needs in real time.\"\n",
            "4. **Cost-Effective Solutions**: \"Maximize your budgetâ€”start with our free tier and scale as your data demands grow.\"\n",
            "5. **Ready, Set, Scrape!**: \"Simplify your data extraction process and fuel your machine learning models with high-quality, structured information.\"\n",
            "\n",
            "These concise copy ideas focus on the benefits and unique selling points of Firecrawl, appealing directly to its target audience.\n"
          ]
        }
      ],
      "source": [
        "print(response.messages[0][\"content\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wGpjmAT91pmd"
      },
      "source": [
        "# Copywriter Women Response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HzzdyRxQ1jQz"
      },
      "outputs": [],
      "source": [
        "response = client.run(agent=copywriter_agent_women, messages=struct)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OsqrbeA61uxU",
        "outputId": "9344a980-cc59-40ab-b112-39b53a9ec5ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1\n"
          ]
        }
      ],
      "source": [
        "print(len(response.messages))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lNIhxMzl1xrE",
        "outputId": "ec213d4a-7b81-4d87-ac9d-504a589f5326"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "**Unlock the Power of Data with Firecrawl!** \n",
            "\n",
            "Hey there, empowered women of innovation! Are you ready to take your AI projects to the next level? Dive into the world of web scraping with Firecrawlâ€”your ultimate partner in transforming chaotic web data into structured, LLM-ready markdown!\n",
            "\n",
            "ğŸŒŸ **Why Choose Firecrawl?** \n",
            "- **Empower Your AI Journey**: Say goodbye to tedious data collection! Firecrawl's seamless scraping capabilities let you focus on what truly mattersâ€”building amazing AI solutions.\n",
            "- **Tailored for You**: Whether youâ€™re a solo developer or leading a team, our user-friendly interface and robust resources cater to every level of expertise. Everyone deserves access to data!\n",
            "- **Harness Dynamic Content**: The web is evolving, and so should your projects! Firecrawl expertly navigates complex, dynamic content, ensuring you have the most relevant data at your fingertips.\n",
            "\n",
            "ğŸŒˆ **Celebrate Your Individuality**: We recognize and cherish the unique perspectives women bring to technology. Firecrawl is designed to support your innovations, making your voice heard in this dynamic field. \n",
            "\n",
            "ğŸ’ª **Join Our Community**: With Firecrawl, youâ€™re not just a user; youâ€™re part of a vibrant network of creators, thinkers, and changemakers! Share your experiences, learn from each other, and empower one another to reach new heights.\n",
            "\n",
            "âœ¨ **Self-Care for Your Projects**: Simplify your workflow and reclaim your time. By choosing Firecrawl, youâ€™re investing in a smoother, more efficient data scraping processâ€”giving you the freedom to focus on self-care and personal growth.\n",
            "\n",
            "â¡ï¸ **Ready to Get Started?** Explore our flexible pricing plans, from a free tier for newcomers to enterprise options for seasoned pros. With Firecrawl, the data you need is just a click away!\n",
            "\n",
            "Join us today and unleash the full potential of your AI projects with Firecrawl! Letâ€™s build a future where your innovations shine! ğŸ’–\n",
            "\n",
            "[Start Your Journey with Firecrawl Now!](https://www.firecrawl.dev/)\n"
          ]
        }
      ],
      "source": [
        "print(response.messages[0][\"content\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzVQA6SH2Itr"
      },
      "source": [
        "# Copywriter Gen Z Response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h8UpHx6rw-v0"
      },
      "outputs": [],
      "source": [
        "response = client.run(agent=copywriter_agent_gen_z, messages=struct)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U1sNmJwJx8A7",
        "outputId": "4109ca2a-75b6-45ad-f9bb-bdc5ded20031"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1\n"
          ]
        }
      ],
      "source": [
        "print(len(response.messages))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Myi-cD4x8Ul",
        "outputId": "7e791df8-76e8-4f5d-bc88-ec42a151df6a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ”¥ Hey there, Data Wizards! ğŸ”¥\n",
            "\n",
            "Are you ready to unleash your AI superpowers? Say hello to **Firecrawl**â€”your one-stop shop for scraping the web like a pro! ğŸš€âœ¨ \n",
            "\n",
            "Why settle for basic when you can have **BIG DATA MAGIC**? ğŸª„ Firecrawl transforms messy website data into clean, LLM-ready markdown thatâ€™ll have your AI projects flourishing like your favorite TikTok dance trends. Itâ€™s like giving your code a glow-up! ğŸŒŸ\n",
            "\n",
            "ğŸ‰ **Whatâ€™s Hot about Firecrawl?** ğŸ‰\n",
            "- **Dynamic Content? No Problem!** ğŸ•º Whether itâ€™s fancy JavaScript or sneaky pop-ups, we handle it all: no ghosting here!\n",
            "- **Open Source Awesomeness**: ğŸ’» Weâ€™re all about that transparency, fam! Contribute to the fire and make it even hotter! \n",
            "- ğŸ”„ **Integration Goals!** Sync it up with your fav toolsâ€”whether youâ€™re Team Python or Team Jupyter, weâ€™ve got you covered! \n",
            "- ğŸ’° **Free Tier Fruity Vibes**: Kick-start your scraping journey with our free plan (yup, you heard that right) for up to 500 pages. \n",
            "\n",
            "Ready to make your data dreams a reality? ğŸŒˆâœ¨ Donâ€™t sleep on thisâ€”tap that button and start your Firecrawl journey NOW! Letâ€™s scrape the web like itâ€™s BLACK FRIDAYâ€”unbeatable deals and zero regrets! ğŸ\n",
            "\n",
            "ğŸ‘‰ [Get started with Firecrawl!](https://www.firecrawl.dev/) \n",
            "\n",
            "P.S. Join the movement! Tag us on socials and flex your Firecrawl skills like you just dropped a new album! ğŸ¤ğŸ’¥ Letâ€™s light up the data scene together! #DataDynamo #WebScrapingMVP #FirecrawlFam\n",
            "\n",
            "Keep it ğŸ”¥ and keep it real,\n",
            "The Firecrawl Team â¤ï¸\n"
          ]
        }
      ],
      "source": [
        "print(response.messages[0][\"content\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xaJwLHVY2TlD"
      },
      "source": [
        "# Copywriter Millenials Response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0DHdGUa-2K_i"
      },
      "outputs": [],
      "source": [
        "response = client.run(agent=copywriter_agent_millennials, messages=struct)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wf9Cyh3z2LjM",
        "outputId": "c2e11c75-5e9a-4314-bd77-5e1a06eccdf3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1\n"
          ]
        }
      ],
      "source": [
        "print(len(response.messages))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3zH3e5YS2LVS",
        "outputId": "8452d537-d99c-4ab0-dfee-9429e9dddbc7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "**Unlock the Power of the Web with Firecrawl! ğŸŒğŸ”¥**\n",
            "\n",
            "Hey there, data dynamos and code wizards! Are you tired of wrestling with messy web data that feels more like a puzzle than a solution? Say goodbye to hours of manual scraping and hello to Firecrawlâ€”the ultimate web scraping superhero for your AI dreams! \n",
            "\n",
            "**Why Firecrawl? Because Swipe Right on Convenience!**  \n",
            "Imagine this: Youâ€™re juggling deadlines like a pro, sipping your matcha latte, when you suddenly realize you need pristine data for your next big project. Thatâ€™s when Firecrawl swoops in, converting websites into sleek, LLM-ready markdown faster than you can say â€œdata-driven decision.â€ ğŸ¦¸â€â™‚ï¸ \n",
            "\n",
            "We're all about helping you turn \"I wish I had time for this\" into \"Wow, I did that!\" because we know how busy life can get. And donâ€™t worry, our solution doesnâ€™t come with a side of frustration. With our easy integration into your existing tools, youâ€™ll feel like the Captain Marvel of data collection. \n",
            "\n",
            "**Quality That Speaks Volumes**  \n",
            "Whether you're dealing with dynamic content or need a data gulp that scalesâ€”the quality isn't just there; itâ€™s off the charts! Partnered with innovators like NVIDIA and Bain, you can trust that our technology is top-tier. Say sayonara to low-quality data that has you second-guessing your insights.\n",
            "\n",
            "**Nostalgic Perfection Meets Financial Savvy**  \n",
            "Remember the thrill of finishing a project ahead of schedule? With Firecrawl, youâ€™ll relive those glory days again! Our free tier allows you to kick the tires for 500 pagesâ€”perfect for those spontaneous side projects or retro flashbacks to your college days. ğŸ“\n",
            "\n",
            "And for the budget-smart thinkers among us, our pricing options mean you can scale up without breaking the bank. More data? Less crying over unexpected expenses. Win-win!\n",
            "\n",
            "**Ready to Level Up Your Data Collection Game?**  \n",
            "Donâ€™t let web data delays hold you back. Click **here** to get started with Firecrawl, where we turn your web scraping nightmares into data dreams! ğŸš€ \n",
            "\n",
            "**Your future self will thank you for making this decision today!** ğŸ‰ #DataSuperhero #Firecrawl #WebScrapingGenius\n",
            "\n",
            "---\n",
            "\n",
            "This messaging captures humor, nostalgia, and the feeling of accomplishment while emphasizing the convenience and quality of Firecrawl's web scraping services. Letâ€™s inspire your audience to take control of their data journey!\n"
          ]
        }
      ],
      "source": [
        "print(response.messages[0][\"content\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rgS3Q65m3jrl"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NRWTx4Al3kST"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
